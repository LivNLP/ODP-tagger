data_dir: ../../Health-outcome-tagger/comet-data/transformer_data/
model_type: bert
model_name_or_path: ../../biobert/biobert_torch/
output_dir: ../../Health-outcome-tagger/output/Fine-tuning/COMET_BIOBERT
labels: ../../Health-outcome-tagger/comet-data/transformer_data/labels.txt
config_name: 
tokenizer_name: 
cache_dir: 
max_seq_length: 171
do_train: True
do_eval: True
do_predict: False
evaluate_during_training: False
do_lower_case: False
per_gpu_train_batch_size: 16
per_gpu_eval_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 5e-05
weight_decay: 0.0
adam_epsilon: 1e-08
max_grad_norm: 1.0
num_train_epochs: 10.0
max_steps: -1
warmup_steps: 0
logging_steps: 50
save_steps: 751
eval_all_checkpoints: False
no_cuda: False
overwrite_output_dir: True
overwrite_cache: False
seed: 1
fp16: False
fp16_opt_level: O1
local_rank: -1
server_ip: 
server_port: 
n_gpu: 2
device: cuda
train_batch_size: 32
